{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07927e79",
   "metadata": {},
   "source": [
    "# Train and evaluate model\n",
    "\n",
    "This notebook recreates Table 1 from the manuscript. We train a basic CNN encoder from scratch on a dataset of sparse polynomials. We then evaluate reconstruction performance on a set of basis systems from across the sciences. \n",
    "\n",
    "Our coding framework is based on a Click interface and we make use of that in this notebook by running the basic steps in the pipeline through shell commands. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e2789a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from phase2vec.utils import get_command_defaults, ensure_dir, write_yaml, update_yaml\n",
    "from phase2vec.train import load_model, train_model, run_epoch\n",
    "from phase2vec.data import load_dataset, SystemFamily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29da8a5a",
   "metadata": {},
   "source": [
    "## Generate data\n",
    "First, we generate both the training and testing sets. The former will be a set of vector fields corresponding to polynomial ODEs of degree at most 3 and having sparse coefficients. The testing set will be vector fields representing the flows of 10 types of sytsems drawn from across the sciences. In all cases, we work with planar (i.e. two-dimensional systems). \n",
    "\n",
    "First, we set some basic parameters, including the types of training and testing data and the number of their samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "851c66a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate data\n",
    "outdir = '../output/' # Alter to change where all of the phase2vec data will be saved. \n",
    "data_dir = os.path.join(outdir, 'data') \n",
    "ensure_dir(data_dir)\n",
    "\n",
    "# Edit the data included in training and testing here. \n",
    "train_data_names = ['polynomial']\n",
    "test_data_names  = ['saddle_node', 'pitchfork', 'transcritical',\n",
    "                    'selkov', 'homoclinic', 'vanderpol',\n",
    "                    'simple_oscillator', 'fitzhugh_nagumo', 'lotka_volterra']\n",
    "\n",
    "# test_data_names  = ['lotka_volterra']\n",
    "train_system_classes = []\n",
    "test_system_classes = []\n",
    "for n, names in enumerate([train_data_names, test_data_names]):\n",
    "    for system in [SystemFamily(data_name=name) for name in names]:\n",
    "        if n == 0:\n",
    "            train_system_classes += [system.data_name + ' ' + str(i) for i in range(len(system.param_groups))]\n",
    "        else:\n",
    "            test_system_classes += [system.data_name + ' ' + str(i) for i in range(len(system.param_groups))]\n",
    "\n",
    "num_train_classes = len(train_system_classes)\n",
    "num_test_classes = len(test_system_classes)\n",
    "\n",
    "# Edit the number of total samples from each data set here.\n",
    "#By default, each set is divied further into a base and validation set at a 75/100 split. This can be altered below. \n",
    "num_train_samples = 100 # total number of train/val samples\n",
    "num_test_samples  = 100 # total number of test samples. Note these are split themselves automatically into a regular and a validation component, but they can be combined. \n",
    "device            = 'cpu' # set to `cpu` if cuda not available\n",
    "\n",
    "# Leave this untouched unless you want to change how parameters from each system are sampled and the proportions of each system in the data set.\n",
    "test_samplers    = ['uniform'] * len(test_data_names)\n",
    "test_props       = [str(1. / len(test_data_names))] * len(test_data_names)\n",
    "test_data_names   = '-s ' +  ' -s '.join(test_data_names)\n",
    "test_samplers     = '-sp ' +  ' -sp '.join(test_samplers)\n",
    "test_props = '-c ' +  ' -c '.join(test_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d881af2c",
   "metadata": {},
   "source": [
    "Next, we call the actual shell commands for generating the data. These commands will make two directories, called `polynomial` and `classical`, corresponding to train and test sets, inside your `data_dir`. \n",
    "\n",
    "In order to alter the validation proportion, $p$, add the flag `--val-size <p>` where $p\\in (0,1)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c484a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.call(f'phase2vec generate-dataset --data-dir {data_dir} --data-set-name classical --num-samples {num_test_samples} {test_data_names} {test_samplers} {test_props}', shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7941d13d",
   "metadata": {},
   "source": [
    "For the training data, we make sure to include the path to the forms for the testing data so that forms are not duplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5942b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_fn = os.path.join(data_dir, 'classical', 'forms.npy')\n",
    "subprocess.call(f'phase2vec generate-dataset --data-dir {data_dir} --num-samples {num_train_samples} --data-set-name polynomial --system-names {train_data_names[0]} -sp control -h {holdout_fn}', shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a948e1",
   "metadata": {},
   "source": [
    "## Instantiate `phase2vec` encoder. \n",
    "\n",
    "We build the embedding CNN. We use the default parameters which we access by fetching the default arguments from the click command `generate_net_config`. To edit these parameters, alter the values of the dictionary `net_info`. \n",
    "\n",
    "* **model_type** (str): which of the pre-built architectures from _models.py to load. Make your own by combining modules from _modules.py \n",
    "* **latent_dim** (int): embedding dimension\n",
    "* Continue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "854d71bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set net parameters\n",
    "from phase2vec.cli._cli import generate_net_config\n",
    "\n",
    "net_info = get_command_defaults(generate_net_config)\n",
    "model_type = net_info['net_class']\n",
    "\n",
    "# These parameters are not considered architectural parameters for the net, so we delete them before they're passed to the net builder. \n",
    "del net_info['net_class']\n",
    "del net_info['output_file']\n",
    "del net_info['pretrained_path']\n",
    "del net_info['ae']\n",
    "\n",
    "net = load_model(model_type, pretrained_path=None, device=device, **net_info).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80583ef7",
   "metadata": {},
   "source": [
    "## Set training parameters and load data. \n",
    "\n",
    "Next, we set the optimization parameters for training. As before, we fetch the default arguments from the relevant click command, `call_train`. These parameters can be updated by altering the values of the dictionary `train_info`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17cca4fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../output/data/polynomial/X_train.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m train_data_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolynomial\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Load training data. \u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m X_train, X_val, y_train, y_val, p_train, p_val \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/phase2vec/phase2vec/data/_utils.py:7\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(data_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_dataset\u001b[39m(data_path):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX_train.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_test.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Load labels\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/phase2vec/lib/python3.8/site-packages/numpy/lib/npyio.py:417\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 417\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    418\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../output/data/polynomial/X_train.npy'"
     ]
    }
   ],
   "source": [
    "## Set training parameters\n",
    "from phase2vec.cli._cli import call_train\n",
    "\n",
    "train_info = get_command_defaults(call_train)\n",
    "train_info['num_epochs'] = 100\n",
    "beta = 1e-3\n",
    "train_info['beta'] = beta\n",
    "train_info['exp_name']   = f'sparse_train_{beta}'\n",
    "\n",
    "# These are only used by the click interface. \n",
    "del train_info['model_save_dir']\n",
    "del train_info['seed']\n",
    "del train_info['config_file']\n",
    "\n",
    "# Set some training paths\n",
    "pretrained_path = None # Replace with model_save_dir in order to load a pretrained model\n",
    "model_save_dir  = os.path.join(outdir, f'models/{train_info[\"exp_name\"]}')\n",
    "ensure_dir(model_save_dir)\n",
    "\n",
    "# Where is training data stored? \n",
    "train_data_path = os.path.join(data_dir, 'polynomial')\n",
    "\n",
    "# Load training data. \n",
    "X_train, X_val, y_train, y_val, p_train, p_val = load_dataset(train_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84192ec",
   "metadata": {},
   "source": [
    "Now, we actually train the model. By default, you can observe training at http://localhost:6007/ and TensorBoard summaries are saved in `train_info['logdir']`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f5a131",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "log_dir = os.path.join(outdir, 'runs')\n",
    "ensure_dir(log_dir)\n",
    "train_info['log_dir'] = log_dir\n",
    "\n",
    "subprocess.call(f'rm -rf {log_dir}/* &', shell=True)\n",
    "subprocess.call(f'tensorboard --logdir {log_dir}&', shell=True)\n",
    "net = train_model(X_train, X_val,\n",
    "                  y_train, y_val,\n",
    "                  p_train, p_val,\n",
    "                  net,**train_info)\n",
    "\n",
    "# Save it\n",
    "from torch import save\n",
    "save(net.state_dict(), os.path.join(model_save_dir, 'model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdc9986",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We evaluate the model and compare it to a per-equation LASSO baseline. First, we load the testing data (putting it all into one big data set) and make sure that function forms between train and test are not duplicated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4d7f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "results_dir = os.path.join(outdir, f'results/{train_info[\"exp_name\"]}')\n",
    "ensure_dir(results_dir)\n",
    "# Load testing data\n",
    "\n",
    "test_data_path = os.path.join(data_dir, 'classical')\n",
    "X_test1, X_test2, y_test1, y_test2, p_test1, p_test2 = load_dataset(test_data_path)\n",
    "X_test = np.concatenate([X_test1, X_test2])\n",
    "y_test = np.concatenate([y_test1, y_test2])\n",
    "p_test = np.concatenate([p_test1, p_test2])\n",
    "\n",
    "# Quickly check for bad forms\n",
    "forms_train = 1 * (p_train != 0)\n",
    "forms_test = np.load(holdout_fn)\n",
    "\n",
    "counter = 0\n",
    "for ftr in forms_train:\n",
    "    bad_form = np.any(np.all(ftr == forms_test))\n",
    "    if bad_form:\n",
    "        counter+=1\n",
    "print(f'{counter} bad forms detected')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5efa661",
   "metadata": {},
   "source": [
    "We write here a couple of helper functions that make it easy to compare `phase2vec` to `LASSO` in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bef8000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_p2v(net, data, **kwargs):\n",
    "    b = data.shape[0]\n",
    "    n = data.shape[2]\n",
    "    emb  = net.emb(net.enc(data).reshape(b, -1))\n",
    "    out  = net.dec(emb)\n",
    "    pars = out.reshape(-1,net.library.shape[-1], 2)\n",
    "    recon = torch.einsum('sl,bld->bsd',net.library.to(device),pars).reshape(b, n,n,2).permute(0,3,1,2)\n",
    "    return pars, recon\n",
    "\n",
    "def forward_lasso(net, data, **kwargs):\n",
    "    b = data.shape[0]\n",
    "    n = data.shape[2]\n",
    "    alpha = kwargs['beta']\n",
    "    # LASSO\n",
    "    pars = []\n",
    "    for z in data:\n",
    "        zx = z[0,:,:].numpy().flatten()\n",
    "        zy = z[1,:,:].numpy().flatten()\n",
    "        clf = linear_model.Lasso(alpha=alpha)\n",
    "        clf.fit(net.library.numpy(), zx)\n",
    "        mx = clf.coef_\n",
    "        clf.fit(net.library.numpy(), zy)\n",
    "        my = clf.coef_\n",
    "        pars.append(torch.stack([torch.tensor(mx), torch.tensor(my)]))\n",
    "    pars = torch.stack(pars).permute(0,2,1)\n",
    "    recon = torch.einsum('sl,bld->bsd',net.library.to(device),pars).reshape(b,n,n,2).permute(0,3,1,2)  \n",
    "    return pars, recon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72808c9a",
   "metadata": {},
   "source": [
    "Finally, we evaluate on a per class basis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d121193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn import linear_model\n",
    "from phase2vec.train._losses import *\n",
    "loss_dict = {}\n",
    "sorted_data = []\n",
    "recon_dict = {'p2v':[],'lasso':[]}\n",
    "fp_normalize = True\n",
    "\n",
    "euclidean = normalized_euclidean if fp_normalize else euclidean_loss\n",
    "\n",
    "# Don't forget to do this. \n",
    "net.eval()\n",
    "for label in tqdm(np.unique(y_test)):\n",
    "\n",
    "    # Data for this class\n",
    "    data   = torch.tensor([datum for (d, datum) in enumerate(X_test) if y_test[d] == label])\n",
    "    pars   = torch.tensor([par for (p, par) in enumerate(p_test) if y_test[p] == label])\n",
    "    labels = torch.ones(len(data)) * label\n",
    "    \n",
    "    sorted_data += list(data)\n",
    "    \n",
    "    # Both the p2v and lasso loss for this class\n",
    "    class_par_losses = []\n",
    "    class_recon_losses = []\n",
    "    # For each fitting method\n",
    "    for nm, forward in zip(recon_dict.keys(),[forward_p2v, forward_lasso]):\n",
    "        \n",
    "        # Fit pars and return recon\n",
    "        pars_fit, recon = forward(net, data.float(), beta=beta)\n",
    "    \n",
    "        # Par loss\n",
    "        par_loss   = euclidean_loss(pars_fit, pars).detach().cpu().numpy()\n",
    "        # Recon loss\n",
    "        recon_loss = euclidean(recon, data).detach().cpu().numpy()\n",
    "        class_par_losses.append(par_loss)\n",
    "        class_recon_losses.append(recon_loss) \n",
    "        \n",
    "        recon_dict[nm] += list(recon)    \n",
    "    loss_dict[test_system_classes[label]] = class_recon_losses + class_par_losses\n",
    "df = pd.DataFrame(data=loss_dict)\n",
    "df.index = [ 'Recon-P2V', 'Recon-LASSO', 'Param-P2V', 'Param-LASSO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9dab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data frame of results\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892bddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show average performance of phase2vec vs lasso\n",
    "df.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1f1ff26",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mp_train\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'p_train' is not defined"
     ]
    }
   ],
   "source": [
    "p_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d975ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
