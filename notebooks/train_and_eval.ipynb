{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07927e79",
   "metadata": {},
   "source": [
    "# Train and evaluate model\n",
    "\n",
    "This notebook recreates Table 1 from the manuscript. We train a basic CNN encoder from scratch on a dataset of sparse polynomials. We then evaluate reconstruction performance on a set of basis systems from across the sciences. \n",
    "\n",
    "Our coding framework is based on a Click interface and we make use of that in this notebook by running the basic steps in the pipeline through shell commands. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e2789a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from src.utils import get_command_defaults, ensure_dir, write_yaml, update_yaml\n",
    "from src.train import load_model, train_model, run_epoch\n",
    "from src.data import load_dataset, SystemFamily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29da8a5a",
   "metadata": {},
   "source": [
    "## Generate data\n",
    "First, we generate both the training and testing sets. The former will be a set of vector fields corresponding to polynomial ODEs of degree at most 3 and having sparse coefficients. The testing set will be vector fields representing the flows of 10 types of sytsems drawn from across the sciences. In all cases, we work with planar (i.e. two-dimensional systems). \n",
    "\n",
    "First, we set some basic parameters, including the types of training and testing data and the number of their samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "851c66a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate data\n",
    "\n",
    "data_dir = '/home/mgricci/data/phase2vec' # Alter to change where all of the phase2vec data will be saved. \n",
    "\n",
    "# Edit the data included in training and testing here. \n",
    "train_data_names = ['polynomial']\n",
    "test_data_names  = ['saddle_node', 'pitchfork', 'transcritical',\n",
    "                    'selkov', 'homoclinic', 'vanderpol',\n",
    "                    'simple_oscillator', 'fitzhugh_nagumo', 'lotka_volterra']\n",
    "\n",
    "# test_data_names  = ['lotka_volterra']\n",
    "train_system_classes = []\n",
    "test_system_classes = []\n",
    "for n, names in enumerate([train_data_names, test_data_names]):\n",
    "    for system in [SystemFamily(data_name=name) for name in names]:\n",
    "        if n == 0:\n",
    "            train_system_classes += [system.data_name + ' ' + str(i) for i in range(len(system.param_groups))]\n",
    "        else:\n",
    "            test_system_classes += [system.data_name + ' ' + str(i) for i in range(len(system.param_groups))]\n",
    "\n",
    "num_train_classes = len(train_system_classes)\n",
    "num_test_classes = len(test_system_classes)\n",
    "\n",
    "# Edit the number of total samples from each data set here.\n",
    "#By default, each set is divied further into a base and validation set at a 75/100 split. This can be altered below. \n",
    "num_train_samples = 10000 # total number of train/val samples\n",
    "num_test_samples  = 2000 # total number of test samples. Note these are split themselves automatically into a regular and a validation component, but they can be combined. \n",
    "device            = 'cpu' # set to `cpu` if cuda not available\n",
    "\n",
    "# Leave this untouched unless you want to change how parameters from each system are sampled and the proportions of each system in the data set.\n",
    "test_samplers    = ['uniform'] * len(test_data_names)\n",
    "test_props       = [str(1. / len(test_data_names))] * len(test_data_names)\n",
    "test_data_names   = '-s ' +  ' -s '.join(test_data_names)\n",
    "test_samplers     = '-sp ' +  ' -sp '.join(test_samplers)\n",
    "test_props = '-c ' +  ' -c '.join(test_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d881af2c",
   "metadata": {},
   "source": [
    "Next, we call the actual shell commands for generating the data. These commands will make two directories, called `polynomial` and `classical`, corresponding to train and test sets, inside your `data_dir`. \n",
    "\n",
    "In order to alter the validation proportion, $p$, add the flag `--val-size <p>` where $p\\in (0,1)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c484a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating saddle_node data.\n",
      "Generating pitchfork data.\n",
      "Generating transcritical data.\n",
      "Generating selkov data.\n",
      "Generating homoclinic data.\n",
      "Generating vanderpol data.\n",
      "Generating simple_oscillator data.\n",
      "Generating fitzhugh_nagumo data.\n",
      "Generating lotka_volterra data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.call(f'phase2vec generate-dataset --data-dir {data_dir} --data-set-name classical --num-samples {num_test_samples} {test_data_names} {test_samplers} {test_props}', shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7941d13d",
   "metadata": {},
   "source": [
    "For the training data, we make sure to include the path to the forms for the testing data so that forms are not duplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5942b437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating polynomial data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holdout_fn = os.path.join(data_dir, 'classical', 'forms.npy')\n",
    "subprocess.call(f'phase2vec generate-dataset --data-dir {data_dir} --num-samples {num_train_samples} --data-set-name polynomial --system-names {train_data_names[0]} -sp control -h {holdout_fn}', shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a948e1",
   "metadata": {},
   "source": [
    "## Instantiate `phase2vec` encoder. \n",
    "\n",
    "We build the embedding CNN. We use the default parameters which we access by fetching the default arguments from the click command `generate_net_config`. To edit these parameters, alter the values of the dictionary `net_info`. \n",
    "\n",
    "* **model_type** (str): which of the pre-built architectures from _models.py to load. Make your own by combining modules from _modules.py \n",
    "* **latent_dim** (int): embedding dimension\n",
    "* Continue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "854d71bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set net parameters\n",
    "from src.cli._cli import generate_net_config\n",
    "\n",
    "net_info = get_command_defaults(generate_net_config)\n",
    "model_type = net_info['net_class']\n",
    "\n",
    "# These parameters are not considered architectural parameters for the net, so we delete them before they're passed to the net builder. \n",
    "del net_info['net_class']\n",
    "del net_info['output_file']\n",
    "del net_info['pretrained_path']\n",
    "del net_info['ae']\n",
    "\n",
    "net = load_model(model_type, pretrained_path=None, device=device, **net_info).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80583ef7",
   "metadata": {},
   "source": [
    "## Set training parameters and load data. \n",
    "\n",
    "Next, we set the optimization parameters for training. As before, we fetch the default arguments from the relevant click command, `call_train`. These parameters can be updated by altering the values of the dictionary `train_info`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17cca4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set training parameters\n",
    "from src.cli._cli import call_train\n",
    "\n",
    "train_info = get_command_defaults(call_train)\n",
    "train_info['log_dir']    = '/home/mgricci/runs'\n",
    "train_info['num_epochs'] = 100\n",
    "beta = 1e-3\n",
    "train_info['beta'] = beta\n",
    "train_info['exp_name']   = f'sparse_train_{beta}'\n",
    "\n",
    "# These are only used by the click interface. \n",
    "del train_info['model_save_dir']\n",
    "del train_info['seed']\n",
    "del train_info['config_file']\n",
    "\n",
    "# Set some training paths\n",
    "\n",
    "pretrained_path = None # Replace with model_save_dir in order to load a pretrained model\n",
    "model_save_dir  = os.path.join('/home/mgricci/phase2vec/', train_info['exp_name'])\n",
    "ensure_dir(model_save_dir)\n",
    "ensure_dir(train_info['log_dir'])\n",
    "\n",
    "# Where is training data stored? \n",
    "train_data_path = os.path.join(data_dir, 'polynomial')\n",
    "\n",
    "# Load training data. \n",
    "X_train, X_val, y_train, y_val, p_train, p_val = load_dataset(train_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84192ec",
   "metadata": {},
   "source": [
    "Now, we actually train the model. By default, you can observe training at http://localhost:6007/ and TensorBoard summaries are saved in `train_info['logdir']`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70f5a131",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.10.1 at http://localhost:6007/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Total loss: 0.68, Recon loss: 0.68,  Sparsity loss: 0.67, Parameter loss: 1.17\n",
      "Validating: Total loss: 0.51, Recon loss: 0.51,  Sparsity loss: 0.63, Parameter loss: 1.14\n",
      "Running Epoch 1\n",
      "Training: Total loss: 0.57, Recon loss: 0.57,  Sparsity loss: 0.71, Parameter loss: 1.17\n",
      "Validating: Total loss: 0.46, Recon loss: 0.46,  Sparsity loss: 0.63, Parameter loss: 1.13\n",
      "Running Epoch 2\n",
      "Training: Total loss: 0.53, Recon loss: 0.53,  Sparsity loss: 0.73, Parameter loss: 1.17\n",
      "Validating: Total loss: 0.43, Recon loss: 0.43,  Sparsity loss: 0.65, Parameter loss: 1.15\n",
      "Running Epoch 3\n",
      "Training: Total loss: 0.51, Recon loss: 0.50,  Sparsity loss: 0.75, Parameter loss: 1.19\n",
      "Validating: Total loss: 0.42, Recon loss: 0.42,  Sparsity loss: 0.67, Parameter loss: 1.16\n",
      "Running Epoch 4\n",
      "Training: Total loss: 0.48, Recon loss: 0.48,  Sparsity loss: 0.77, Parameter loss: 1.21\n",
      "Validating: Total loss: 0.39, Recon loss: 0.39,  Sparsity loss: 0.70, Parameter loss: 1.17\n",
      "Running Epoch 5\n",
      "Training: Total loss: 0.47, Recon loss: 0.47,  Sparsity loss: 0.79, Parameter loss: 1.23\n",
      "Validating: Total loss: 0.38, Recon loss: 0.38,  Sparsity loss: 0.71, Parameter loss: 1.19\n",
      "Running Epoch 6\n",
      "Training: Total loss: 0.46, Recon loss: 0.46,  Sparsity loss: 0.80, Parameter loss: 1.24\n",
      "Validating: Total loss: 0.37, Recon loss: 0.37,  Sparsity loss: 0.70, Parameter loss: 1.19\n",
      "Running Epoch 7\n",
      "Training: Total loss: 0.45, Recon loss: 0.45,  Sparsity loss: 0.81, Parameter loss: 1.25\n",
      "Validating: Total loss: 0.35, Recon loss: 0.35,  Sparsity loss: 0.73, Parameter loss: 1.22\n",
      "Running Epoch 8\n",
      "Training: Total loss: 0.44, Recon loss: 0.44,  Sparsity loss: 0.82, Parameter loss: 1.26\n",
      "Validating: Total loss: 0.35, Recon loss: 0.35,  Sparsity loss: 0.74, Parameter loss: 1.23\n",
      "Running Epoch 9\n",
      "Training: Total loss: 0.43, Recon loss: 0.43,  Sparsity loss: 0.83, Parameter loss: 1.27\n",
      "Validating: Total loss: 0.33, Recon loss: 0.33,  Sparsity loss: 0.74, Parameter loss: 1.23\n",
      "Running Epoch 10\n",
      "Training: Total loss: 0.43, Recon loss: 0.43,  Sparsity loss: 0.83, Parameter loss: 1.27\n",
      "Validating: Total loss: 0.33, Recon loss: 0.33,  Sparsity loss: 0.74, Parameter loss: 1.23\n",
      "Running Epoch 11\n",
      "Training: Total loss: 0.42, Recon loss: 0.42,  Sparsity loss: 0.84, Parameter loss: 1.28\n",
      "Validating: Total loss: 0.33, Recon loss: 0.33,  Sparsity loss: 0.74, Parameter loss: 1.24\n",
      "Running Epoch 12\n",
      "Training: Total loss: 0.42, Recon loss: 0.42,  Sparsity loss: 0.85, Parameter loss: 1.29\n",
      "Validating: Total loss: 0.33, Recon loss: 0.33,  Sparsity loss: 0.74, Parameter loss: 1.24\n",
      "Running Epoch 13\n",
      "Training: Total loss: 0.42, Recon loss: 0.42,  Sparsity loss: 0.85, Parameter loss: 1.29\n",
      "Validating: Total loss: 0.33, Recon loss: 0.33,  Sparsity loss: 0.74, Parameter loss: 1.24\n",
      "Running Epoch 14\n",
      "Training: Total loss: 0.41, Recon loss: 0.41,  Sparsity loss: 0.85, Parameter loss: 1.29\n",
      "Validating: Total loss: 0.32, Recon loss: 0.32,  Sparsity loss: 0.74, Parameter loss: 1.24\n",
      "Running Epoch 15\n",
      "Training: Total loss: 0.41, Recon loss: 0.41,  Sparsity loss: 0.86, Parameter loss: 1.30\n",
      "Validating: Total loss: 0.32, Recon loss: 0.32,  Sparsity loss: 0.77, Parameter loss: 1.26\n",
      "Running Epoch 16\n",
      "Training: Total loss: 0.41, Recon loss: 0.41,  Sparsity loss: 0.86, Parameter loss: 1.30\n",
      "Validating: Total loss: 0.31, Recon loss: 0.31,  Sparsity loss: 0.75, Parameter loss: 1.25\n",
      "Running Epoch 17\n",
      "Training: Total loss: 0.40, Recon loss: 0.40,  Sparsity loss: 0.86, Parameter loss: 1.30\n",
      "Validating: Total loss: 0.32, Recon loss: 0.32,  Sparsity loss: 0.75, Parameter loss: 1.25\n",
      "Running Epoch 18\n",
      "Training: Total loss: 0.40, Recon loss: 0.40,  Sparsity loss: 0.86, Parameter loss: 1.30\n",
      "Validating: Total loss: 0.31, Recon loss: 0.31,  Sparsity loss: 0.77, Parameter loss: 1.26\n",
      "Running Epoch 19\n",
      "Training: Total loss: 0.40, Recon loss: 0.40,  Sparsity loss: 0.87, Parameter loss: 1.30\n",
      "Validating: Total loss: 0.31, Recon loss: 0.31,  Sparsity loss: 0.76, Parameter loss: 1.26\n",
      "Running Epoch 20\n",
      "Training: Total loss: 0.40, Recon loss: 0.40,  Sparsity loss: 0.87, Parameter loss: 1.30\n",
      "Validating: Total loss: 0.30, Recon loss: 0.30,  Sparsity loss: 0.77, Parameter loss: 1.26\n",
      "Running Epoch 21\n",
      "Training: Total loss: 0.40, Recon loss: 0.40,  Sparsity loss: 0.87, Parameter loss: 1.31\n",
      "Validating: Total loss: 0.30, Recon loss: 0.30,  Sparsity loss: 0.76, Parameter loss: 1.26\n",
      "Running Epoch 22\n",
      "Training: Total loss: 0.40, Recon loss: 0.39,  Sparsity loss: 0.88, Parameter loss: 1.31\n",
      "Validating: Total loss: 0.30, Recon loss: 0.30,  Sparsity loss: 0.77, Parameter loss: 1.26\n",
      "Running Epoch 23\n",
      "Training: Total loss: 0.39, Recon loss: 0.39,  Sparsity loss: 0.88, Parameter loss: 1.31\n",
      "Validating: Total loss: 0.29, Recon loss: 0.29,  Sparsity loss: 0.77, Parameter loss: 1.27\n",
      "Running Epoch 24\n",
      "Training: Total loss: 0.39, Recon loss: 0.39,  Sparsity loss: 0.88, Parameter loss: 1.32\n",
      "Validating: Total loss: 0.30, Recon loss: 0.30,  Sparsity loss: 0.78, Parameter loss: 1.27\n",
      "Running Epoch 25\n",
      "Training: Total loss: 0.39, Recon loss: 0.39,  Sparsity loss: 0.89, Parameter loss: 1.32\n",
      "Validating: Total loss: 0.29, Recon loss: 0.29,  Sparsity loss: 0.78, Parameter loss: 1.27\n",
      "Running Epoch 26\n",
      "Training: Total loss: 0.39, Recon loss: 0.39,  Sparsity loss: 0.89, Parameter loss: 1.32\n",
      "Validating: Total loss: 0.30, Recon loss: 0.30,  Sparsity loss: 0.78, Parameter loss: 1.27\n",
      "Running Epoch 27\n",
      "Training: Total loss: 0.38, Recon loss: 0.38,  Sparsity loss: 0.89, Parameter loss: 1.32\n",
      "Validating: Total loss: 0.29, Recon loss: 0.29,  Sparsity loss: 0.78, Parameter loss: 1.27\n",
      "Running Epoch 28\n",
      "Training: Total loss: 0.38, Recon loss: 0.38,  Sparsity loss: 0.90, Parameter loss: 1.33\n",
      "Validating: Total loss: 0.28, Recon loss: 0.28,  Sparsity loss: 0.80, Parameter loss: 1.28\n",
      "Running Epoch 29\n",
      "Training: Total loss: 0.38, Recon loss: 0.38,  Sparsity loss: 0.90, Parameter loss: 1.33\n",
      "Validating: Total loss: 0.28, Recon loss: 0.28,  Sparsity loss: 0.80, Parameter loss: 1.29\n",
      "Running Epoch 30\n",
      "Training: Total loss: 0.38, Recon loss: 0.38,  Sparsity loss: 0.91, Parameter loss: 1.33\n",
      "Validating: Total loss: 0.28, Recon loss: 0.28,  Sparsity loss: 0.79, Parameter loss: 1.28\n",
      "Running Epoch 31\n",
      "Training: Total loss: 0.38, Recon loss: 0.38,  Sparsity loss: 0.91, Parameter loss: 1.33\n",
      "Validating: Total loss: 0.29, Recon loss: 0.28,  Sparsity loss: 0.79, Parameter loss: 1.27\n",
      "Running Epoch 32\n",
      "Training: Total loss: 0.37, Recon loss: 0.37,  Sparsity loss: 0.92, Parameter loss: 1.34\n",
      "Validating: Total loss: 0.28, Recon loss: 0.28,  Sparsity loss: 0.81, Parameter loss: 1.28\n",
      "Running Epoch 33\n",
      "Training: Total loss: 0.38, Recon loss: 0.38,  Sparsity loss: 0.92, Parameter loss: 1.34\n",
      "Validating: Total loss: 0.27, Recon loss: 0.27,  Sparsity loss: 0.81, Parameter loss: 1.29\n",
      "Running Epoch 34\n",
      "Training: Total loss: 0.37, Recon loss: 0.37,  Sparsity loss: 0.92, Parameter loss: 1.34\n",
      "Validating: Total loss: 0.27, Recon loss: 0.27,  Sparsity loss: 0.81, Parameter loss: 1.29\n",
      "Running Epoch 35\n",
      "Training: Total loss: 0.37, Recon loss: 0.37,  Sparsity loss: 0.93, Parameter loss: 1.34\n",
      "Validating: Total loss: 0.28, Recon loss: 0.28,  Sparsity loss: 0.81, Parameter loss: 1.28\n",
      "Running Epoch 36\n",
      "Training: Total loss: 0.37, Recon loss: 0.37,  Sparsity loss: 0.93, Parameter loss: 1.34\n",
      "Validating: Total loss: 0.27, Recon loss: 0.26,  Sparsity loss: 0.83, Parameter loss: 1.30\n",
      "Running Epoch 37\n",
      "Training: Total loss: 0.37, Recon loss: 0.37,  Sparsity loss: 0.93, Parameter loss: 1.34\n",
      "Validating: Total loss: 0.27, Recon loss: 0.27,  Sparsity loss: 0.82, Parameter loss: 1.29\n",
      "Running Epoch 38\n",
      "Training: Total loss: 0.37, Recon loss: 0.37,  Sparsity loss: 0.94, Parameter loss: 1.35\n",
      "Validating: Total loss: 0.27, Recon loss: 0.27,  Sparsity loss: 0.83, Parameter loss: 1.29\n",
      "Running Epoch 39\n",
      "Training: Total loss: 0.37, Recon loss: 0.37,  Sparsity loss: 0.94, Parameter loss: 1.35\n",
      "Validating: Total loss: 0.27, Recon loss: 0.26,  Sparsity loss: 0.83, Parameter loss: 1.29\n",
      "Running Epoch 40\n",
      "Training: Total loss: 0.37, Recon loss: 0.36,  Sparsity loss: 0.94, Parameter loss: 1.35\n",
      "Validating: Total loss: 0.27, Recon loss: 0.27,  Sparsity loss: 0.82, Parameter loss: 1.30\n",
      "Running Epoch 41\n",
      "Training: Total loss: 0.37, Recon loss: 0.36,  Sparsity loss: 0.95, Parameter loss: 1.35\n",
      "Validating: Total loss: 0.26, Recon loss: 0.26,  Sparsity loss: 0.83, Parameter loss: 1.30\n",
      "Running Epoch 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Total loss: 0.36, Recon loss: 0.36,  Sparsity loss: 0.95, Parameter loss: 1.35\n",
      "Validating: Total loss: 0.27, Recon loss: 0.27,  Sparsity loss: 0.83, Parameter loss: 1.30\n",
      "Running Epoch 43\n",
      "Training: Total loss: 0.36, Recon loss: 0.36,  Sparsity loss: 0.95, Parameter loss: 1.36\n",
      "Validating: Total loss: 0.26, Recon loss: 0.26,  Sparsity loss: 0.83, Parameter loss: 1.30\n",
      "Running Epoch 44\n",
      "Training: Total loss: 0.36, Recon loss: 0.36,  Sparsity loss: 0.96, Parameter loss: 1.36\n",
      "Validating: Total loss: 0.26, Recon loss: 0.26,  Sparsity loss: 0.84, Parameter loss: 1.30\n",
      "Running Epoch 45\n",
      "Training: Total loss: 0.36, Recon loss: 0.36,  Sparsity loss: 0.96, Parameter loss: 1.36\n",
      "Validating: Total loss: 0.25, Recon loss: 0.25,  Sparsity loss: 0.84, Parameter loss: 1.30\n",
      "Running Epoch 46\n",
      "Training: Total loss: 0.36, Recon loss: 0.36,  Sparsity loss: 0.96, Parameter loss: 1.36\n",
      "Validating: Total loss: 0.26, Recon loss: 0.26,  Sparsity loss: 0.84, Parameter loss: 1.30\n",
      "Running Epoch 47\n",
      "Training: Total loss: 0.36, Recon loss: 0.36,  Sparsity loss: 0.96, Parameter loss: 1.36\n",
      "Validating: Total loss: 0.26, Recon loss: 0.26,  Sparsity loss: 0.84, Parameter loss: 1.30\n",
      "Running Epoch 48\n",
      "Training: Total loss: 0.36, Recon loss: 0.36,  Sparsity loss: 0.97, Parameter loss: 1.37\n",
      "Validating: Total loss: 0.25, Recon loss: 0.25,  Sparsity loss: 0.85, Parameter loss: 1.32\n",
      "Running Epoch 49\n",
      "Training: Total loss: 0.36, Recon loss: 0.36,  Sparsity loss: 0.97, Parameter loss: 1.37\n",
      "Validating: Total loss: 0.26, Recon loss: 0.25,  Sparsity loss: 0.86, Parameter loss: 1.30\n",
      "Running Epoch 50\n",
      "Training: Total loss: 0.35, Recon loss: 0.35,  Sparsity loss: 0.97, Parameter loss: 1.37\n",
      "Validating: Total loss: 0.25, Recon loss: 0.25,  Sparsity loss: 0.85, Parameter loss: 1.31\n",
      "Running Epoch 51\n",
      "Training: Total loss: 0.35, Recon loss: 0.35,  Sparsity loss: 0.97, Parameter loss: 1.37\n",
      "Validating: Total loss: 0.26, Recon loss: 0.25,  Sparsity loss: 0.84, Parameter loss: 1.30\n",
      "Running Epoch 52\n",
      "Training: Total loss: 0.35, Recon loss: 0.35,  Sparsity loss: 0.98, Parameter loss: 1.37\n",
      "Validating: Total loss: 0.25, Recon loss: 0.25,  Sparsity loss: 0.86, Parameter loss: 1.31\n",
      "Running Epoch 53\n",
      "Training: Total loss: 0.35, Recon loss: 0.35,  Sparsity loss: 0.98, Parameter loss: 1.37\n",
      "Validating: Total loss: 0.25, Recon loss: 0.25,  Sparsity loss: 0.85, Parameter loss: 1.30\n",
      "Running Epoch 54\n",
      "Training: Total loss: 0.35, Recon loss: 0.35,  Sparsity loss: 0.98, Parameter loss: 1.37\n",
      "Validating: Total loss: 0.24, Recon loss: 0.24,  Sparsity loss: 0.87, Parameter loss: 1.33\n",
      "Running Epoch 55\n",
      "Training: Total loss: 0.35, Recon loss: 0.35,  Sparsity loss: 0.98, Parameter loss: 1.38\n",
      "Validating: Total loss: 0.24, Recon loss: 0.24,  Sparsity loss: 0.86, Parameter loss: 1.32\n",
      "Running Epoch 56\n",
      "Training: Total loss: 0.35, Recon loss: 0.35,  Sparsity loss: 0.99, Parameter loss: 1.38\n",
      "Validating: Total loss: 0.25, Recon loss: 0.25,  Sparsity loss: 0.86, Parameter loss: 1.31\n",
      "Running Epoch 57\n",
      "Training: Total loss: 0.35, Recon loss: 0.35,  Sparsity loss: 0.99, Parameter loss: 1.38\n",
      "Validating: Total loss: 0.24, Recon loss: 0.24,  Sparsity loss: 0.87, Parameter loss: 1.31\n",
      "Running Epoch 58\n",
      "Training: Total loss: 0.35, Recon loss: 0.35,  Sparsity loss: 0.99, Parameter loss: 1.38\n",
      "Validating: Total loss: 0.24, Recon loss: 0.24,  Sparsity loss: 0.87, Parameter loss: 1.32\n",
      "Running Epoch 59\n",
      "Training: Total loss: 0.35, Recon loss: 0.35,  Sparsity loss: 0.99, Parameter loss: 1.39\n",
      "Validating: Total loss: 0.25, Recon loss: 0.25,  Sparsity loss: 0.86, Parameter loss: 1.31\n",
      "Running Epoch 60\n",
      "Training: Total loss: 0.35, Recon loss: 0.35,  Sparsity loss: 1.00, Parameter loss: 1.39\n",
      "Validating: Total loss: 0.24, Recon loss: 0.24,  Sparsity loss: 0.86, Parameter loss: 1.31\n",
      "Running Epoch 61\n",
      "Training: Total loss: 0.35, Recon loss: 0.35,  Sparsity loss: 1.00, Parameter loss: 1.39\n",
      "Validating: Total loss: 0.24, Recon loss: 0.24,  Sparsity loss: 0.87, Parameter loss: 1.32\n",
      "Running Epoch 62\n",
      "Training: Total loss: 0.34, Recon loss: 0.34,  Sparsity loss: 1.00, Parameter loss: 1.39\n",
      "Validating: Total loss: 0.24, Recon loss: 0.24,  Sparsity loss: 0.86, Parameter loss: 1.32\n",
      "Running Epoch 63\n",
      "Training: Total loss: 0.34, Recon loss: 0.34,  Sparsity loss: 1.00, Parameter loss: 1.39\n",
      "Validating: Total loss: 0.24, Recon loss: 0.24,  Sparsity loss: 0.87, Parameter loss: 1.32\n",
      "Running Epoch 64\n",
      "Training: Total loss: 0.34, Recon loss: 0.34,  Sparsity loss: 1.01, Parameter loss: 1.40\n",
      "Validating: Total loss: 0.24, Recon loss: 0.24,  Sparsity loss: 0.87, Parameter loss: 1.32\n",
      "Running Epoch 65\n",
      "Training: Total loss: 0.34, Recon loss: 0.34,  Sparsity loss: 1.01, Parameter loss: 1.40\n",
      "Validating: Total loss: 0.23, Recon loss: 0.23,  Sparsity loss: 0.88, Parameter loss: 1.34\n",
      "Running Epoch 66\n",
      "Training: Total loss: 0.34, Recon loss: 0.34,  Sparsity loss: 1.01, Parameter loss: 1.40\n",
      "Validating: Total loss: 0.23, Recon loss: 0.23,  Sparsity loss: 0.88, Parameter loss: 1.33\n",
      "Running Epoch 67\n",
      "Training: Total loss: 0.34, Recon loss: 0.34,  Sparsity loss: 1.01, Parameter loss: 1.40\n",
      "Validating: Total loss: 0.24, Recon loss: 0.24,  Sparsity loss: 0.86, Parameter loss: 1.32\n",
      "Running Epoch 68\n",
      "Training: Total loss: 0.34, Recon loss: 0.34,  Sparsity loss: 1.01, Parameter loss: 1.40\n",
      "Validating: Total loss: 0.23, Recon loss: 0.23,  Sparsity loss: 0.87, Parameter loss: 1.32\n",
      "Running Epoch 69\n",
      "Training: Total loss: 0.34, Recon loss: 0.34,  Sparsity loss: 1.01, Parameter loss: 1.40\n",
      "Validating: Total loss: 0.24, Recon loss: 0.23,  Sparsity loss: 0.88, Parameter loss: 1.33\n",
      "Running Epoch 70\n",
      "Training: Total loss: 0.34, Recon loss: 0.34,  Sparsity loss: 1.02, Parameter loss: 1.40\n",
      "Validating: Total loss: 0.23, Recon loss: 0.22,  Sparsity loss: 0.89, Parameter loss: 1.34\n",
      "Running Epoch 71\n",
      "Training: Total loss: 0.34, Recon loss: 0.34,  Sparsity loss: 1.02, Parameter loss: 1.41\n",
      "Validating: Total loss: 0.23, Recon loss: 0.23,  Sparsity loss: 0.88, Parameter loss: 1.34\n",
      "Running Epoch 72\n",
      "Training: Total loss: 0.34, Recon loss: 0.34,  Sparsity loss: 1.02, Parameter loss: 1.41\n",
      "Validating: Total loss: 0.23, Recon loss: 0.23,  Sparsity loss: 0.89, Parameter loss: 1.34\n",
      "Running Epoch 73\n",
      "Training: Total loss: 0.34, Recon loss: 0.34,  Sparsity loss: 1.02, Parameter loss: 1.41\n",
      "Validating: Total loss: 0.23, Recon loss: 0.23,  Sparsity loss: 0.88, Parameter loss: 1.33\n",
      "Running Epoch 74\n",
      "Training: Total loss: 0.34, Recon loss: 0.34,  Sparsity loss: 1.02, Parameter loss: 1.41\n",
      "Validating: Total loss: 0.23, Recon loss: 0.23,  Sparsity loss: 0.88, Parameter loss: 1.33\n",
      "Running Epoch 75\n",
      "Training: Total loss: 0.34, Recon loss: 0.34,  Sparsity loss: 1.02, Parameter loss: 1.41\n",
      "Validating: Total loss: 0.23, Recon loss: 0.23,  Sparsity loss: 0.88, Parameter loss: 1.34\n",
      "Running Epoch 76\n",
      "Training: Total loss: 0.34, Recon loss: 0.34,  Sparsity loss: 1.03, Parameter loss: 1.41\n",
      "Validating: Total loss: 0.23, Recon loss: 0.23,  Sparsity loss: 0.88, Parameter loss: 1.33\n",
      "Running Epoch 77\n",
      "Training: Total loss: 0.34, Recon loss: 0.34,  Sparsity loss: 1.02, Parameter loss: 1.41\n",
      "Validating: Total loss: 0.23, Recon loss: 0.23,  Sparsity loss: 0.88, Parameter loss: 1.33\n",
      "Running Epoch 78\n",
      "Training: Total loss: 0.34, Recon loss: 0.33,  Sparsity loss: 1.03, Parameter loss: 1.41\n",
      "Validating: Total loss: 0.23, Recon loss: 0.23,  Sparsity loss: 0.89, Parameter loss: 1.34\n",
      "Running Epoch 79\n",
      "Training: Total loss: 0.34, Recon loss: 0.33,  Sparsity loss: 1.03, Parameter loss: 1.42\n",
      "Validating: Total loss: 0.23, Recon loss: 0.23,  Sparsity loss: 0.88, Parameter loss: 1.34\n",
      "Running Epoch 80\n",
      "Training: Total loss: 0.33, Recon loss: 0.33,  Sparsity loss: 1.03, Parameter loss: 1.42\n",
      "Validating: Total loss: 0.22, Recon loss: 0.22,  Sparsity loss: 0.89, Parameter loss: 1.34\n",
      "Running Epoch 81\n",
      "Training: Total loss: 0.33, Recon loss: 0.33,  Sparsity loss: 1.03, Parameter loss: 1.42\n",
      "Validating: Total loss: 0.22, Recon loss: 0.22,  Sparsity loss: 0.89, Parameter loss: 1.34\n",
      "Running Epoch 82\n",
      "Training: Total loss: 0.33, Recon loss: 0.33,  Sparsity loss: 1.03, Parameter loss: 1.42\n",
      "Validating: Total loss: 0.22, Recon loss: 0.22,  Sparsity loss: 0.90, Parameter loss: 1.35\n",
      "Running Epoch 83\n",
      "Training: Total loss: 0.33, Recon loss: 0.33,  Sparsity loss: 1.03, Parameter loss: 1.42\n",
      "Validating: Total loss: 0.22, Recon loss: 0.22,  Sparsity loss: 0.89, Parameter loss: 1.34\n",
      "Running Epoch 84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Total loss: 0.33, Recon loss: 0.33,  Sparsity loss: 1.04, Parameter loss: 1.42\n",
      "Validating: Total loss: 0.22, Recon loss: 0.22,  Sparsity loss: 0.89, Parameter loss: 1.35\n",
      "Running Epoch 85\n",
      "Training: Total loss: 0.33, Recon loss: 0.33,  Sparsity loss: 1.04, Parameter loss: 1.42\n",
      "Validating: Total loss: 0.22, Recon loss: 0.22,  Sparsity loss: 0.88, Parameter loss: 1.33\n",
      "Running Epoch 86\n",
      "Training: Total loss: 0.33, Recon loss: 0.33,  Sparsity loss: 1.04, Parameter loss: 1.43\n",
      "Validating: Total loss: 0.22, Recon loss: 0.22,  Sparsity loss: 0.89, Parameter loss: 1.34\n",
      "Running Epoch 87\n",
      "Training: Total loss: 0.33, Recon loss: 0.33,  Sparsity loss: 1.04, Parameter loss: 1.43\n",
      "Validating: Total loss: 0.22, Recon loss: 0.22,  Sparsity loss: 0.89, Parameter loss: 1.34\n",
      "Running Epoch 88\n",
      "Training: Total loss: 0.33, Recon loss: 0.33,  Sparsity loss: 1.04, Parameter loss: 1.43\n",
      "Validating: Total loss: 0.22, Recon loss: 0.22,  Sparsity loss: 0.90, Parameter loss: 1.36\n",
      "Running Epoch 89\n",
      "Training: Total loss: 0.33, Recon loss: 0.33,  Sparsity loss: 1.04, Parameter loss: 1.44\n",
      "Validating: Total loss: 0.23, Recon loss: 0.23,  Sparsity loss: 0.89, Parameter loss: 1.35\n",
      "Running Epoch 90\n",
      "Training: Total loss: 0.33, Recon loss: 0.33,  Sparsity loss: 1.04, Parameter loss: 1.43\n",
      "Validating: Total loss: 0.21, Recon loss: 0.21,  Sparsity loss: 0.91, Parameter loss: 1.36\n",
      "Running Epoch 91\n",
      "Training: Total loss: 0.33, Recon loss: 0.33,  Sparsity loss: 1.05, Parameter loss: 1.43\n",
      "Validating: Total loss: 0.22, Recon loss: 0.22,  Sparsity loss: 0.89, Parameter loss: 1.34\n",
      "Running Epoch 92\n",
      "Training: Total loss: 0.33, Recon loss: 0.33,  Sparsity loss: 1.05, Parameter loss: 1.44\n",
      "Validating: Total loss: 0.23, Recon loss: 0.23,  Sparsity loss: 0.89, Parameter loss: 1.35\n",
      "Running Epoch 93\n",
      "Training: Total loss: 0.33, Recon loss: 0.33,  Sparsity loss: 1.05, Parameter loss: 1.43\n",
      "Validating: Total loss: 0.22, Recon loss: 0.22,  Sparsity loss: 0.89, Parameter loss: 1.34\n",
      "Running Epoch 94\n",
      "Training: Total loss: 0.33, Recon loss: 0.33,  Sparsity loss: 1.05, Parameter loss: 1.44\n",
      "Validating: Total loss: 0.22, Recon loss: 0.22,  Sparsity loss: 0.90, Parameter loss: 1.36\n",
      "Running Epoch 95\n",
      "Training: Total loss: 0.33, Recon loss: 0.33,  Sparsity loss: 1.05, Parameter loss: 1.44\n",
      "Validating: Total loss: 0.22, Recon loss: 0.22,  Sparsity loss: 0.90, Parameter loss: 1.36\n",
      "Running Epoch 96\n",
      "Training: Total loss: 0.33, Recon loss: 0.33,  Sparsity loss: 1.05, Parameter loss: 1.44\n",
      "Validating: Total loss: 0.22, Recon loss: 0.22,  Sparsity loss: 0.90, Parameter loss: 1.35\n",
      "Running Epoch 97\n",
      "Training: Total loss: 0.33, Recon loss: 0.33,  Sparsity loss: 1.05, Parameter loss: 1.44\n",
      "Validating: Total loss: 0.22, Recon loss: 0.22,  Sparsity loss: 0.89, Parameter loss: 1.35\n",
      "Running Epoch 98\n",
      "Training: Total loss: 0.33, Recon loss: 0.33,  Sparsity loss: 1.05, Parameter loss: 1.45\n",
      "Validating: Total loss: 0.22, Recon loss: 0.22,  Sparsity loss: 0.90, Parameter loss: 1.36\n",
      "Running Epoch 99\n",
      "Training: Total loss: 0.33, Recon loss: 0.32,  Sparsity loss: 1.06, Parameter loss: 1.44\n",
      "Validating: Total loss: 0.21, Recon loss: 0.21,  Sparsity loss: 0.90, Parameter loss: 1.36\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "log_dir = train_info['log_dir']\n",
    "subprocess.call(f'rm -rf {log_dir}/* &', shell=True)\n",
    "subprocess.call(f'tensorboard --logdir {log_dir}&', shell=True)\n",
    "net = train_model(X_train, X_val,\n",
    "                  y_train, y_val,\n",
    "                  p_train, p_val,\n",
    "                  net,**train_info)\n",
    "\n",
    "# Save it\n",
    "from torch import save\n",
    "save(net.state_dict(), os.path.join(model_save_dir, 'model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdc9986",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We evaluate the model and compare it to a per-equation LASSO baseline. First, we load the testing data (putting it all into one big data set) and make sure that function forms between train and test are not duplicated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb4d7f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad forms detected\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "results_dir = f'/home/mgricci/results/phase2vec/{train_info[\"exp_name\"]}'\n",
    "ensure_dir(results_dir)\n",
    "# Load testing data\n",
    "\n",
    "test_data_path = os.path.join(data_dir, 'classical')\n",
    "X_test1, X_test2, y_test1, y_test2, p_test1, p_test2 = load_dataset(test_data_path)\n",
    "X_test = np.concatenate([X_test1, X_test2])\n",
    "y_test = np.concatenate([y_test1, y_test2])\n",
    "p_test = np.concatenate([p_test1, p_test2])\n",
    "\n",
    "# Quickly check for bad forms\n",
    "forms_train = 1 * (p_train != 0)\n",
    "forms_test = np.load(holdout_fn)\n",
    "\n",
    "counter = 0\n",
    "for ftr in forms_train:\n",
    "    bad_form = np.any(np.all(ftr == forms_test))\n",
    "    if bad_form:\n",
    "        counter+=1\n",
    "print(f'{counter} bad forms detected')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5efa661",
   "metadata": {},
   "source": [
    "We write here a couple of helper functions that make it easy to compare `phase2vec` to `LASSO` in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bef8000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_p2v(net, data, **kwargs):\n",
    "    b = data.shape[0]\n",
    "    n = data.shape[2]\n",
    "    emb  = net.emb(net.enc(data).reshape(b, -1))\n",
    "    out  = net.dec(emb)\n",
    "    pars = out.reshape(-1,net.library.shape[-1], 2)\n",
    "    recon = torch.einsum('sl,bld->bsd',net.library.to(device),pars).reshape(b, n,n,2).permute(0,3,1,2)\n",
    "    return pars, recon\n",
    "\n",
    "def forward_lasso(net, data, **kwargs):\n",
    "    b = data.shape[0]\n",
    "    n = data.shape[2]\n",
    "    alpha = kwargs['beta']\n",
    "    # LASSO\n",
    "    pars = []\n",
    "    for z in data:\n",
    "        zx = z[0,:,:].numpy().flatten()\n",
    "        zy = z[1,:,:].numpy().flatten()\n",
    "        clf = linear_model.Lasso(alpha=alpha)\n",
    "        clf.fit(net.library.numpy(), zx)\n",
    "        mx = clf.coef_\n",
    "        clf.fit(net.library.numpy(), zy)\n",
    "        my = clf.coef_\n",
    "        pars.append(torch.stack([torch.tensor(mx), torch.tensor(my)]))\n",
    "    pars = torch.stack(pars).permute(0,2,1)\n",
    "    recon = torch.einsum('sl,bld->bsd',net.library.to(device),pars).reshape(b,n,n,2).permute(0,3,1,2)  \n",
    "    return pars, recon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72808c9a",
   "metadata": {},
   "source": [
    "Finally, we evaluate on a per class basis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d121193a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:07<00:00,  2.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn import linear_model\n",
    "from src.train._losses import *\n",
    "import pdb\n",
    "loss_dict = {}\n",
    "sorted_data = []\n",
    "recon_dict = {'p2v':[],'lasso':[]}\n",
    "fp_normalize = True\n",
    "\n",
    "euclidean = normalized_euclidean if fp_normalize else euclidean_loss\n",
    "\n",
    "# Don't forget to do this. \n",
    "net.eval()\n",
    "for label in tqdm(np.unique(y_test)):\n",
    "\n",
    "    # Data for this class\n",
    "    data   = torch.tensor([datum for (d, datum) in enumerate(X_test) if y_test[d] == label])\n",
    "    pars   = torch.tensor([par for (p, par) in enumerate(p_test) if y_test[p] == label])\n",
    "    labels = torch.ones(len(data)) * label\n",
    "    \n",
    "    sorted_data += list(data)\n",
    "    \n",
    "    # Both the p2v and lasso loss for this class\n",
    "    class_par_losses = []\n",
    "    class_recon_losses = []\n",
    "    # For each fitting method\n",
    "    for nm, forward in zip(recon_dict.keys(),[forward_p2v, forward_lasso]):\n",
    "        \n",
    "        # Fit pars and return recon\n",
    "        pars_fit, recon = forward(net, data.float(), beta=beta)\n",
    "    \n",
    "        # Par loss\n",
    "        par_loss   = euclidean_loss(pars_fit, pars).detach().cpu().numpy()\n",
    "        # Recon loss\n",
    "        recon_loss = euclidean(recon, data).detach().cpu().numpy()\n",
    "        class_par_losses.append(par_loss)\n",
    "        class_recon_losses.append(recon_loss) \n",
    "        \n",
    "        recon_dict[nm] += list(recon)    \n",
    "    loss_dict[test_system_classes[label]] = class_recon_losses + class_par_losses\n",
    "df = pd.DataFrame(data=loss_dict)\n",
    "df.index = [ 'Recon-P2V', 'Recon-LASSO', 'Param-P2V', 'Param-LASSO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab9dab68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>saddle_node 0</th>\n",
       "      <th>saddle_node 1</th>\n",
       "      <th>pitchfork 0</th>\n",
       "      <th>pitchfork 1</th>\n",
       "      <th>transcritical 0</th>\n",
       "      <th>transcritical 1</th>\n",
       "      <th>selkov 0</th>\n",
       "      <th>selkov 1</th>\n",
       "      <th>homoclinic 0</th>\n",
       "      <th>homoclinic 1</th>\n",
       "      <th>vanderpol 0</th>\n",
       "      <th>simple_oscillator 0</th>\n",
       "      <th>simple_oscillator 1</th>\n",
       "      <th>fitzhugh_nagumo 0</th>\n",
       "      <th>fitzhugh_nagumo 1</th>\n",
       "      <th>lotka_volterra 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Recon-P2V</th>\n",
       "      <td>0.10267034</td>\n",
       "      <td>0.20150924</td>\n",
       "      <td>0.12904318</td>\n",
       "      <td>0.2266368</td>\n",
       "      <td>0.17027943</td>\n",
       "      <td>0.14567259</td>\n",
       "      <td>0.81592095</td>\n",
       "      <td>0.67325145</td>\n",
       "      <td>0.22962801</td>\n",
       "      <td>0.20151435</td>\n",
       "      <td>0.3188012</td>\n",
       "      <td>0.12414951</td>\n",
       "      <td>0.16118951</td>\n",
       "      <td>0.43614286</td>\n",
       "      <td>0.43157324</td>\n",
       "      <td>1.8828284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recon-LASSO</th>\n",
       "      <td>0.36323085</td>\n",
       "      <td>0.526674</td>\n",
       "      <td>0.0024045436</td>\n",
       "      <td>0.011648577</td>\n",
       "      <td>0.0041172192</td>\n",
       "      <td>0.0041190884</td>\n",
       "      <td>3.355808</td>\n",
       "      <td>2.6324751</td>\n",
       "      <td>0.0023160803</td>\n",
       "      <td>0.0023286792</td>\n",
       "      <td>0.0023888485</td>\n",
       "      <td>0.0039446712</td>\n",
       "      <td>0.014330661</td>\n",
       "      <td>0.14821275</td>\n",
       "      <td>0.43089703</td>\n",
       "      <td>2.0078204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Param-P2V</th>\n",
       "      <td>0.17810035</td>\n",
       "      <td>0.17712435</td>\n",
       "      <td>0.1887456</td>\n",
       "      <td>0.14538993</td>\n",
       "      <td>0.18924163</td>\n",
       "      <td>0.18316503</td>\n",
       "      <td>6.089389</td>\n",
       "      <td>6.1224647</td>\n",
       "      <td>3.1989481</td>\n",
       "      <td>2.8069565</td>\n",
       "      <td>10.276819</td>\n",
       "      <td>0.55033225</td>\n",
       "      <td>0.4937132</td>\n",
       "      <td>6.1399918</td>\n",
       "      <td>6.1376195</td>\n",
       "      <td>0.4663909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Param-LASSO</th>\n",
       "      <td>0.2164217</td>\n",
       "      <td>0.21830283</td>\n",
       "      <td>0.23080356</td>\n",
       "      <td>0.22881134</td>\n",
       "      <td>0.23543648</td>\n",
       "      <td>0.23557638</td>\n",
       "      <td>6.591731</td>\n",
       "      <td>6.4733195</td>\n",
       "      <td>3.627736</td>\n",
       "      <td>3.1080275</td>\n",
       "      <td>15.782345</td>\n",
       "      <td>0.65094006</td>\n",
       "      <td>0.63297683</td>\n",
       "      <td>11.633111</td>\n",
       "      <td>11.64577</td>\n",
       "      <td>0.70657265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            saddle_node 0 saddle_node 1   pitchfork 0  pitchfork 1  \\\n",
       "Recon-P2V      0.10267034    0.20150924    0.12904318    0.2266368   \n",
       "Recon-LASSO    0.36323085      0.526674  0.0024045436  0.011648577   \n",
       "Param-P2V      0.17810035    0.17712435     0.1887456   0.14538993   \n",
       "Param-LASSO     0.2164217    0.21830283    0.23080356   0.22881134   \n",
       "\n",
       "            transcritical 0 transcritical 1    selkov 0    selkov 1  \\\n",
       "Recon-P2V        0.17027943      0.14567259  0.81592095  0.67325145   \n",
       "Recon-LASSO    0.0041172192    0.0041190884    3.355808   2.6324751   \n",
       "Param-P2V        0.18924163      0.18316503    6.089389   6.1224647   \n",
       "Param-LASSO      0.23543648      0.23557638    6.591731   6.4733195   \n",
       "\n",
       "             homoclinic 0  homoclinic 1   vanderpol 0 simple_oscillator 0  \\\n",
       "Recon-P2V      0.22962801    0.20151435     0.3188012          0.12414951   \n",
       "Recon-LASSO  0.0023160803  0.0023286792  0.0023888485        0.0039446712   \n",
       "Param-P2V       3.1989481     2.8069565     10.276819          0.55033225   \n",
       "Param-LASSO      3.627736     3.1080275     15.782345          0.65094006   \n",
       "\n",
       "            simple_oscillator 1 fitzhugh_nagumo 0 fitzhugh_nagumo 1  \\\n",
       "Recon-P2V            0.16118951        0.43614286        0.43157324   \n",
       "Recon-LASSO         0.014330661        0.14821275        0.43089703   \n",
       "Param-P2V             0.4937132         6.1399918         6.1376195   \n",
       "Param-LASSO          0.63297683         11.633111          11.64577   \n",
       "\n",
       "            lotka_volterra 0  \n",
       "Recon-P2V          1.8828284  \n",
       "Recon-LASSO        2.0078204  \n",
       "Param-P2V          0.4663909  \n",
       "Param-LASSO       0.70657265  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "892bddc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Recon-P2V      0.390676\n",
       "Recon-LASSO    0.594545\n",
       "Param-P2V      2.709025\n",
       "Param-LASSO    3.888618\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac01268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
